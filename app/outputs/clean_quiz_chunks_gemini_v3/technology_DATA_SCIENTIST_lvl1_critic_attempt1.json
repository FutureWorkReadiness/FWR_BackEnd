{
  "quiz_pool": [
    {
      "id": 1,
      "question": "Which of the following tasks is typically NOT performed by a data scientist in their daily work?",
      "options": [
        {
          "key": "A",
          "text": "Developing machine learning models to predict customer behavior and market trends using advanced algorithms.",
          "is_correct": false,
          "rationale": "Developing ML models is a core data science task."
        },
        {
          "key": "B",
          "text": "Collecting, cleaning, and preprocessing data from various sources to prepare it for analysis and modeling.",
          "is_correct": false,
          "rationale": "Data wrangling is a fundamental data science task."
        },
        {
          "key": "C",
          "text": "Maintaining and administering the company's database infrastructure, including managing user access rights and permissions.",
          "is_correct": true,
          "rationale": "Database administration is usually a separate role."
        },
        {
          "key": "D",
          "text": "Communicating findings and insights to stakeholders through data visualizations and comprehensive reports, ensuring clarity.",
          "is_correct": false,
          "rationale": "Communicating insights is a key data science task."
        },
        {
          "key": "E",
          "text": "Designing and implementing experiments to test hypotheses and validate the performance of machine learning models effectively.",
          "is_correct": false,
          "rationale": "Experiment design is a core data science task."
        }
      ]
    },
    {
      "id": 2,
      "question": "Which statistical method is primarily employed for the purpose of dimensionality reduction in data analysis?",
      "options": [
        {
          "key": "A",
          "text": "Linear Regression, a method used for predicting continuous variables based on the relationships between input features.",
          "is_correct": false,
          "rationale": "Linear regression is for prediction, not dimensionality reduction."
        },
        {
          "key": "B",
          "text": "Principal Component Analysis (PCA), a technique for transforming data into a set of uncorrelated components.",
          "is_correct": true,
          "rationale": "PCA is a common dimensionality reduction technique."
        },
        {
          "key": "C",
          "text": "Analysis of Variance (ANOVA), a method for comparing means across different groups to assess statistical significance.",
          "is_correct": false,
          "rationale": "ANOVA compares means, not performs dimensionality reduction."
        },
        {
          "key": "D",
          "text": "Time Series Analysis, a specialized method for forecasting future values based on historical data patterns and trends.",
          "is_correct": false,
          "rationale": "Time series analysis is for forecasting, not reduction."
        },
        {
          "key": "E",
          "text": "Logistic Regression, a technique for predicting binary outcomes based on input features and observed data patterns.",
          "is_correct": false,
          "rationale": "Logistic regression is for binary classification."
        }
      ]
    },
    {
      "id": 3,
      "question": "What is the primary purpose of employing cross-validation techniques during machine learning model development and evaluation?",
      "options": [
        {
          "key": "A",
          "text": "To accelerate the training process by strategically reducing the amount of data utilized for model training purposes.",
          "is_correct": false,
          "rationale": "Cross-validation uses all data, not a reduced subset."
        },
        {
          "key": "B",
          "text": "To accurately estimate the performance of a model on unseen data, effectively mitigating potential overfitting issues.",
          "is_correct": true,
          "rationale": "Cross-validation estimates generalization performance."
        },
        {
          "key": "C",
          "text": "To enhance the complexity of a model by incorporating additional features and intricate interaction effects within the model.",
          "is_correct": false,
          "rationale": "Cross-validation does not affect model complexity."
        },
        {
          "key": "D",
          "text": "To visually represent the decision boundaries of a model, facilitating a deeper understanding of its overall behavior.",
          "is_correct": false,
          "rationale": "Cross-validation is not designed for visualization."
        },
        {
          "key": "E",
          "text": "To diminish the variance of the model by aggregating the predictions derived from multiple models within the ensemble.",
          "is_correct": false,
          "rationale": "Ensemble methods reduce variance, not cross-validation."
        }
      ]
    },
    {
      "id": 4,
      "question": "Which of the following represents a commonly employed technique for effectively handling missing data points within a dataset?",
      "options": [
        {
          "key": "A",
          "text": "Replacing missing values with the mean, median, or mode calculated from the available data points in the dataset.",
          "is_correct": true,
          "rationale": "Imputation is a common method for handling missing data."
        },
        {
          "key": "B",
          "text": "Normalizing the data to a standardized range between 0 and 1 using the min-max scaling technique.",
          "is_correct": false,
          "rationale": "Normalization scales data, but is not for missing values."
        },
        {
          "key": "C",
          "text": "Encoding categorical variables into numerical representations using the one-hot encoding technique for machine learning.",
          "is_correct": false,
          "rationale": "Encoding converts categories to numbers, not for missing values."
        },
        {
          "key": "D",
          "text": "Standardizing the data to achieve a mean of 0 and a standard deviation of 1 across all features.",
          "is_correct": false,
          "rationale": "Standardization scales data, but is not for missing values."
        },
        {
          "key": "E",
          "text": "Employing feature selection methods to identify and choose the most relevant features for the predictive model.",
          "is_correct": false,
          "rationale": "Feature selection chooses important features, not for missing values."
        }
      ]
    },
    {
      "id": 5,
      "question": "What is the fundamental purpose of applying regularization techniques in machine learning models and algorithm design?",
      "options": [
        {
          "key": "A",
          "text": "To effectively prevent overfitting by introducing a penalty term to the loss function, thereby reducing parameter sizes.",
          "is_correct": true,
          "rationale": "Regularization prevents overfitting by penalizing complexity."
        },
        {
          "key": "B",
          "text": "To enhance the complexity of the model by incorporating additional layers or nodes within the neural network architecture.",
          "is_correct": false,
          "rationale": "Regularization reduces model complexity, not increases it."
        },
        {
          "key": "C",
          "text": "To accelerate the training process by strategically reducing the amount of data used for model training purposes.",
          "is_correct": false,
          "rationale": "Regularization does not alter the size of the training data."
        },
        {
          "key": "D",
          "text": "To enhance the interpretability of the model by simplifying the decision boundaries and making them more understandable.",
          "is_correct": false,
          "rationale": "Regularization affects model coefficients, not boundaries directly."
        },
        {
          "key": "E",
          "text": "To diminish the variance of the model by aggregating the predictions from multiple models into a single ensemble prediction.",
          "is_correct": false,
          "rationale": "Ensemble methods reduce variance, not regularization directly."
        }
      ]
    },
    {
      "id": 6,
      "question": "Which of the following evaluation metrics is commonly utilized to assess the performance of classification models?",
      "options": [
        {
          "key": "A",
          "text": "R-squared, a metric that quantifies the proportion of variance in the dependent variable explained by the model.",
          "is_correct": false,
          "rationale": "R-squared is primarily used for regression models."
        },
        {
          "key": "B",
          "text": "Mean Squared Error (MSE), a metric that calculates the average squared difference between predicted and actual values.",
          "is_correct": false,
          "rationale": "MSE is typically used for regression tasks."
        },
        {
          "key": "C",
          "text": "Root Mean Squared Error (RMSE), which measures the square root of the average of the squared differences.",
          "is_correct": false,
          "rationale": "RMSE is used for regression, not classification."
        },
        {
          "key": "D",
          "text": "Accuracy, a metric that quantifies the proportion of instances that are correctly classified by the model.",
          "is_correct": true,
          "rationale": "Accuracy is a common metric for classification models."
        },
        {
          "key": "E",
          "text": "Mean Absolute Error (MAE), a metric that calculates the average absolute difference between predictions and actual values.",
          "is_correct": false,
          "rationale": "MAE is generally used for regression problems."
        }
      ]
    },
    {
      "id": 7,
      "question": "What is the primary objective of applying feature scaling techniques like standardization or normalization to a dataset?",
      "options": [
        {
          "key": "A",
          "text": "To reduce the number of features in the dataset by strategically selecting the most relevant features for modeling.",
          "is_correct": false,
          "rationale": "Feature selection reduces the number of features."
        },
        {
          "key": "B",
          "text": "To convert categorical variables into numerical representations, enabling their use in machine learning models and algorithms.",
          "is_correct": false,
          "rationale": "Encoding handles categorical features, not scaling."
        },
        {
          "key": "C",
          "text": "To ensure that all features have a similar range of values, preventing features with larger values from dominating analysis.",
          "is_correct": true,
          "rationale": "Scaling ensures equal feature importance and prevents dominance."
        },
        {
          "key": "D",
          "text": "To effectively handle missing data by imputing values based on the distribution of the available data points in the dataset.",
          "is_correct": false,
          "rationale": "Imputation handles missing data, not feature scaling."
        },
        {
          "key": "E",
          "text": "To reduce the dimensionality of the dataset by projecting the data onto a lower-dimensional space, simplifying the model.",
          "is_correct": false,
          "rationale": "Dimensionality reduction reduces the number of dimensions."
        }
      ]
    },
    {
      "id": 8,
      "question": "Which of the following algorithms is categorized as a type of unsupervised learning algorithm in machine learning?",
      "options": [
        {
          "key": "A",
          "text": "Linear Regression, an algorithm that predicts a continuous output variable based on input features and their relationships.",
          "is_correct": false,
          "rationale": "Linear regression is a supervised learning algorithm."
        },
        {
          "key": "B",
          "text": "Logistic Regression, an algorithm that predicts a binary output based on input features and their associated probabilities.",
          "is_correct": false,
          "rationale": "Logistic Regression is a supervised learning algorithm."
        },
        {
          "key": "C",
          "text": "Decision Tree, an algorithm that creates a tree-like structure to classify or predict outcomes based on decision rules.",
          "is_correct": false,
          "rationale": "Decision trees can be used for supervised learning."
        },
        {
          "key": "D",
          "text": "K-Means Clustering, an algorithm that groups similar data points together based on distance measures and cluster analysis.",
          "is_correct": true,
          "rationale": "K-means is an unsupervised clustering algorithm."
        },
        {
          "key": "E",
          "text": "Support Vector Machine (SVM), an algorithm that finds the optimal hyperplane to separate data points into different classes.",
          "is_correct": false,
          "rationale": "SVM is a supervised learning algorithm."
        }
      ]
    },
    {
      "id": 9,
      "question": "In the context of machine learning, what does the term 'overfitting' generally refer to when discussing model performance?",
      "options": [
        {
          "key": "A",
          "text": "A model that exhibits excellent performance on the training data but performs poorly when applied to unseen data.",
          "is_correct": true,
          "rationale": "Overfitting implies poor generalization to new data."
        },
        {
          "key": "B",
          "text": "A model that is overly simplistic and fails to capture the underlying patterns and complexities present in the data.",
          "is_correct": false,
          "rationale": "This scenario describes underfitting, not overfitting."
        },
        {
          "key": "C",
          "text": "A model that requires an excessively long time to train due to the large size and complexity of the dataset.",
          "is_correct": false,
          "rationale": "Training time is not directly related to overfitting."
        },
        {
          "key": "D",
          "text": "A model that is unable to converge to a stable and reliable solution during the training process and optimization.",
          "is_correct": false,
          "rationale": "Lack of convergence is a separate issue from overfitting."
        },
        {
          "key": "E",
          "text": "A model that achieves perfect accuracy on both the training dataset and the testing dataset, indicating ideal performance.",
          "is_correct": false,
          "rationale": "Perfect accuracy is rare and does not indicate overfitting."
        }
      ]
    },
    {
      "id": 10,
      "question": "Which of the following options represents a commonly used data visualization library within the Python programming ecosystem?",
      "options": [
        {
          "key": "A",
          "text": "NumPy, a library that provides comprehensive support for numerical computations and efficient array manipulation in Python.",
          "is_correct": false,
          "rationale": "NumPy is primarily for numerical computation, not visualization."
        },
        {
          "key": "B",
          "text": "Pandas, a library that offers powerful data structures and versatile tools for data analysis and manipulation tasks.",
          "is_correct": false,
          "rationale": "Pandas is for data manipulation, not direct visualization."
        },
        {
          "key": "C",
          "text": "Scikit-learn, a library that provides a wide range of machine learning algorithms and tools for model evaluation and selection.",
          "is_correct": false,
          "rationale": "Scikit-learn is for machine learning, not general visualization."
        },
        {
          "key": "D",
          "text": "Matplotlib, a library that offers a diverse set of plotting functions for creating various types of visualizations in Python.",
          "is_correct": true,
          "rationale": "Matplotlib is a widely used visualization library in Python."
        },
        {
          "key": "E",
          "text": "Requests, a library that provides tools and functionalities for making HTTP requests to interact with web services and APIs.",
          "is_correct": false,
          "rationale": "Requests is designed for web requests, not visualization."
        }
      ]
    },
    {
      "id": 11,
      "question": "What is the primary purpose of utilizing a confusion matrix in the context of classification tasks and model evaluation?",
      "options": [
        {
          "key": "A",
          "text": "To visually represent the distribution of data points within a high-dimensional space, aiding in data exploration.",
          "is_correct": false,
          "rationale": "A confusion matrix is not designed for data distribution visualization."
        },
        {
          "key": "B",
          "text": "To evaluate the performance of a classification model by displaying the counts of true positives, false positives, and related metrics.",
          "is_correct": true,
          "rationale": "A confusion matrix evaluates classification performance effectively."
        },
        {
          "key": "C",
          "text": "To identify the most important features within a dataset for prediction, guiding feature selection and engineering processes.",
          "is_correct": false,
          "rationale": "A confusion matrix is not used for feature selection purposes."
        },
        {
          "key": "D",
          "text": "To measure the correlation between different variables within a dataset, revealing relationships and dependencies among them.",
          "is_correct": false,
          "rationale": "A confusion matrix is not designed for correlation analysis."
        },
        {
          "key": "E",
          "text": "To reduce the dimensionality of a dataset by projecting the data onto a lower-dimensional space, simplifying the model.",
          "is_correct": false,
          "rationale": "A confusion matrix is not used for dimensionality reduction."
        }
      ]
    },
    {
      "id": 12,
      "question": "Which of the following techniques is commonly employed to address the challenges posed by imbalanced datasets in machine learning?",
      "options": [
        {
          "key": "A",
          "text": "Oversampling the minority class by generating synthetic samples or duplicating existing samples to balance class distribution.",
          "is_correct": true,
          "rationale": "Oversampling effectively balances the class distribution in imbalanced datasets."
        },
        {
          "key": "B",
          "text": "Underfitting the model intentionally to prevent it from capturing the noise present in the data, sacrificing accuracy.",
          "is_correct": false,
          "rationale": "Underfitting is not a suitable technique for handling imbalanced data."
        },
        {
          "key": "C",
          "text": "Normalizing the data to a standardized range between 0 and 1 using min-max scaling to improve model performance.",
          "is_correct": false,
          "rationale": "Normalization is not specifically designed for imbalanced data."
        },
        {
          "key": "D",
          "text": "Standardizing the data to achieve a mean of 0 and a standard deviation of 1 across all features for consistency.",
          "is_correct": false,
          "rationale": "Standardization is not a primary technique for imbalanced data."
        },
        {
          "key": "E",
          "text": "Employing feature selection methods to identify and choose the most relevant features for the predictive model's training.",
          "is_correct": false,
          "rationale": "Feature selection is not directly related to imbalanced data handling."
        }
      ]
    },
    {
      "id": 13,
      "question": "What is the fundamental purpose of conducting a hypothesis test in the realm of statistical analysis and inference?",
      "options": [
        {
          "key": "A",
          "text": "To estimate the parameters of a population based on the information obtained from a sample drawn from that population.",
          "is_correct": false,
          "rationale": "Parameter estimation is a distinct statistical task."
        },
        {
          "key": "B",
          "text": "To determine whether there is sufficient statistical evidence to reject a null hypothesis in favor of an alternative hypothesis.",
          "is_correct": true,
          "rationale": "Hypothesis tests assess evidence against the null hypothesis."
        },
        {
          "key": "C",
          "text": "To visually represent the relationship between two variables using a scatter plot, revealing patterns and trends in the data.",
          "is_correct": false,
          "rationale": "Scatter plots are used for visualizing relationships."
        },
        {
          "key": "D",
          "text": "To calculate the correlation coefficient between two variables, quantifying the strength and direction of their linear association.",
          "is_correct": false,
          "rationale": "Correlation measures the association between variables."
        },
        {
          "key": "E",
          "text": "To reduce the dimensionality of a dataset by projecting the data onto a lower-dimensional space, simplifying the model.",
          "is_correct": false,
          "rationale": "Dimensionality reduction reduces the number of dimensions."
        }
      ]
    },
    {
      "id": 14,
      "question": "Which of the following options is NOT considered a standard step within the Cross-Industry Standard Process for Data Mining (CRISP-DM) methodology?",
      "options": [
        {
          "key": "A",
          "text": "Data Understanding, which involves thoroughly exploring the data and identifying its key characteristics and potential issues.",
          "is_correct": false,
          "rationale": "Data understanding is a core step in the CRISP-DM process."
        },
        {
          "key": "B",
          "text": "Data Preparation, which involves cleaning, transforming, and integrating the data to make it suitable for modeling purposes.",
          "is_correct": false,
          "rationale": "Data preparation is a crucial step in CRISP-DM."
        },
        {
          "key": "C",
          "text": "Model Building, which involves selecting and training appropriate machine learning models based on the prepared data.",
          "is_correct": false,
          "rationale": "Model building is a central step in the CRISP-DM process."
        },
        {
          "key": "D",
          "text": "Model Deployment, which involves deploying the trained model into a production environment for real-world application and use.",
          "is_correct": false,
          "rationale": "Model deployment is a key step in the CRISP-DM cycle."
        },
        {
          "key": "E",
          "text": "Business Ideation, which involves brainstorming new business ideas and strategies unrelated to the data mining process itself.",
          "is_correct": true,
          "rationale": "Business ideation is not a standard part of the CRISP-DM methodology."
        }
      ]
    },
    {
      "id": 15,
      "question": "What is the fundamental difference that distinguishes supervised learning from unsupervised learning approaches in machine learning?",
      "options": [
        {
          "key": "A",
          "text": "Supervised learning utilizes labeled data for training, whereas unsupervised learning uses unlabeled data to discover patterns.",
          "is_correct": true,
          "rationale": "Supervised learning requires labeled data for training."
        },
        {
          "key": "B",
          "text": "Supervised learning is primarily used for classification tasks, while unsupervised learning is used for regression problems.",
          "is_correct": false,
          "rationale": "Both can be used for classification and regression tasks."
        },
        {
          "key": "C",
          "text": "Supervised learning is used for clustering data points, while unsupervised learning is used for dimensionality reduction techniques.",
          "is_correct": false,
          "rationale": "Clustering and dimensionality reduction are unsupervised techniques."
        },
        {
          "key": "D",
          "text": "Supervised learning is used for feature selection, while unsupervised learning is used for feature engineering and transformation.",
          "is_correct": false,
          "rationale": "Feature selection and engineering are not tied to either paradigm."
        },
        {
          "key": "E",
          "text": "Supervised learning is used for time series analysis, while unsupervised learning is used for natural language processing tasks.",
          "is_correct": false,
          "rationale": "Both can be applied to NLP and time series analysis."
        }
      ]
    },
    {
      "id": 16,
      "question": "Which of the following represents a common data storage solution that is well-suited for handling and managing large datasets effectively?",
      "options": [
        {
          "key": "A",
          "text": "Microsoft Excel, a spreadsheet program designed for organizing and analyzing data in a tabular format with limited capacity.",
          "is_correct": false,
          "rationale": "Excel is not suitable for very large datasets due to limitations."
        },
        {
          "key": "B",
          "text": "CSV files, which are plain text files that store data in a comma-separated format, suitable for smaller datasets.",
          "is_correct": false,
          "rationale": "CSV files can be used, but are not ideal for very large datasets."
        },
        {
          "key": "C",
          "text": "Relational databases (e.g., MySQL, PostgreSQL), which organize data into tables with rows and columns, ensuring structure.",
          "is_correct": true,
          "rationale": "Relational databases are well-suited for structured data storage."
        },
        {
          "key": "D",
          "text": "JSON files, which are human-readable text files that store data in a key-value pair format, often used for configuration.",
          "is_correct": false,
          "rationale": "JSON files can be used, but are not optimal for very large datasets."
        },
        {
          "key": "E",
          "text": "TXT files, which are plain text files that store data without any specific formatting or structure, limiting analysis.",
          "is_correct": false,
          "rationale": "TXT files are not suitable for structured data storage and analysis."
        }
      ]
    },
    {
      "id": 17,
      "question": "What is the primary purpose and objective of conducting exploratory data analysis (EDA) on a given dataset?",
      "options": [
        {
          "key": "A",
          "text": "To build and train machine learning models for prediction and classification tasks, leveraging the data's patterns.",
          "is_correct": false,
          "rationale": "EDA is not primarily focused on model building."
        },
        {
          "key": "B",
          "text": "To clean and preprocess the data by removing missing values and outliers, ensuring data quality for modeling.",
          "is_correct": false,
          "rationale": "EDA is not solely for data cleaning and preprocessing."
        },
        {
          "key": "C",
          "text": "To summarize the main characteristics of a dataset and gain insights into its structure, distributions, and relationships.",
          "is_correct": true,
          "rationale": "EDA is primarily for understanding the data's characteristics."
        },
        {
          "key": "D",
          "text": "To deploy machine learning models into a production environment for real-world application and use by end-users.",
          "is_correct": false,
          "rationale": "EDA is not related to model deployment processes."
        },
        {
          "key": "E",
          "text": "To evaluate the performance of machine learning models using metrics such as accuracy and precision, assessing effectiveness.",
          "is_correct": false,
          "rationale": "EDA is not for model evaluation or performance assessment."
        }
      ]
    },
    {
      "id": 18,
      "question": "What specific role does the p-value play in the context of hypothesis testing within statistical analysis?",
      "options": [
        {
          "key": "A",
          "text": "The probability of observing results as extreme as, or more extreme than, the observed results if the null hypothesis is true.",
          "is_correct": true,
          "rationale": "The p-value represents the probability of the data given the null."
        },
        {
          "key": "B",
          "text": "The probability of rejecting the null hypothesis when it is actually false, indicating the test's sensitivity.",
          "is_correct": false,
          "rationale": "This describes the concept of statistical power, not the p-value."
        },
        {
          "key": "C",
          "text": "The probability of failing to reject the null hypothesis when it is actually false, leading to a missed discovery.",
          "is_correct": false,
          "rationale": "This describes a Type II error, not the p-value itself."
        },
        {
          "key": "D",
          "text": "The probability of the null hypothesis being true given the observed data, reflecting the likelihood of the null.",
          "is_correct": false,
          "rationale": "This is a common misinterpretation of the p-value."
        },
        {
          "key": "E",
          "text": "The probability of accepting the null hypothesis when it is actually true, indicating the test's reliability.",
          "is_correct": false,
          "rationale": "This is not a standard or meaningful statistical concept."
        }
      ]
    },
    {
      "id": 19,
      "question": "Which Python library is generally considered the most suitable and comprehensive for performing in-depth statistical analysis?",
      "options": [
        {
          "key": "A",
          "text": "Scikit-learn, a library that primarily focuses on machine learning algorithms and model evaluation metrics for predictive modeling.",
          "is_correct": false,
          "rationale": "Scikit-learn is primarily for machine learning, not general statistics."
        },
        {
          "key": "B",
          "text": "TensorFlow, a library widely used for deep learning and neural network computations, particularly in complex AI systems.",
          "is_correct": false,
          "rationale": "TensorFlow is specialized for deep learning and neural networks."
        },
        {
          "key": "C",
          "text": "Statsmodels, a library that provides classes and functions for estimating statistical models, conducting tests, and exploring data.",
          "is_correct": true,
          "rationale": "Statsmodels is specifically designed for statistical analysis."
        },
        {
          "key": "D",
          "text": "Requests, a library used for making HTTP requests to interact with web services and APIs, enabling data retrieval.",
          "is_correct": false,
          "rationale": "Requests is designed for web requests, not statistical analysis."
        },
        {
          "key": "E",
          "text": "Keras, a high-level API for building and training neural networks, simplifying the development of deep learning models.",
          "is_correct": false,
          "rationale": "Keras is a high-level API for neural networks and deep learning."
        }
      ]
    },
    {
      "id": 20,
      "question": "What is the primary purpose and objective of feature engineering within the context of machine learning model development?",
      "options": [
        {
          "key": "A",
          "text": "To select the most relevant features from a dataset for model training, improving efficiency and reducing complexity.",
          "is_correct": false,
          "rationale": "Feature selection focuses on choosing relevant features, not engineering."
        },
        {
          "key": "B",
          "text": "To transform raw data into features that better represent the underlying problem, enhancing model performance and accuracy.",
          "is_correct": true,
          "rationale": "Feature engineering aims to improve model performance through transformation."
        },
        {
          "key": "C",
          "text": "To reduce the dimensionality of a dataset by projecting the data onto a lower-dimensional space, simplifying the model structure.",
          "is_correct": false,
          "rationale": "Dimensionality reduction reduces the number of dimensions."
        },
        {
          "key": "D",
          "text": "To evaluate the performance of machine learning models using metrics such as accuracy and precision, assessing effectiveness.",
          "is_correct": false,
          "rationale": "Model evaluation assesses performance, not feature engineering."
        },
        {
          "key": "E",
          "text": "To clean and preprocess the data by removing missing values and outliers, ensuring data quality and consistency for modeling.",
          "is_correct": false,
          "rationale": "Data cleaning handles missing values and outliers, not feature engineering."
        }
      ]
    }
  ]
}